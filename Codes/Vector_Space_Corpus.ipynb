{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Importing libraries"],"metadata":{"id":"g_LN9cDH6o_I"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","import string\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import stopwords\n","import re\n","import numpy as np\n","import math"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbLgiKJL2G-V","outputId":"d5ede23d-6946-43ab-e32a-605a1db73f93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# Reading Corpus"],"metadata":{"id":"Xjh6dGil6roE"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulx8FR1FK37p","outputId":"843ca353-95b0-49d4-8cb6-941079123d95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1QscBtuFj3XW7Gl3lhWdEx6r8Yit0yJ2v\n","To: /content/nfcorpus.tar.gz\n","100% 97.3M/97.3M [00:00<00:00, 156MB/s]\n"]}],"source":["# NFCorpus\n","!gdown 1QscBtuFj3XW7Gl3lhWdEx6r8Yit0yJ2v"]},{"cell_type":"code","source":["!tar -xvf 'nfcorpus.tar.gz'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4LvygNEK5BO","outputId":"c17fec4f-f390-4abc-eb3a-49dc6b3f7393"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nfcorpus/\n","nfcorpus/train.docs\n","nfcorpus/test.docs\n","nfcorpus/dev.docs\n","nfcorpus/dev.3-2-1.qrel\n","nfcorpus/test.3-2-1.qrel\n","nfcorpus/train.3-2-1.qrel\n","nfcorpus/raw/\n","nfcorpus/raw/doc_dump.txt\n","nfcorpus/raw/dev.docs.ids\n","nfcorpus/raw/dev.queries.ids\n","nfcorpus/raw/test.docs.ids\n","nfcorpus/raw/test.queries.ids\n","nfcorpus/raw/train.docs.ids\n","nfcorpus/raw/train.queries.ids\n","nfcorpus/raw/stopwords.large\n","nfcorpus/raw/nfdump.txt\n","nfcorpus/raw/all_videos.ids\n","nfcorpus/raw/nontopics.ids\n","nfcorpus/test.2-1-0.qrel\n","nfcorpus/dev.2-1-0.qrel\n","nfcorpus/train.2-1-0.qrel\n","nfcorpus/ecir2016.bib\n","nfcorpus/test.vid-desc.queries\n","nfcorpus/dev.vid-desc.queries\n","nfcorpus/train.vid-desc.queries\n","nfcorpus/dev.all.queries\n","nfcorpus/test.all.queries\n","nfcorpus/train.all.queries\n","nfcorpus/dev.vid-titles.queries\n","nfcorpus/test.vid-titles.queries\n","nfcorpus/train.vid-titles.queries\n","nfcorpus/dev.titles.queries\n","nfcorpus/test.titles.queries\n","nfcorpus/train.titles.queries\n","nfcorpus/dev.nontopic-titles.queries\n","nfcorpus/test.nontopic-titles.queries\n","nfcorpus/train.nontopic-titles.queries\n","nfcorpus/README.txt\n"]}]},{"cell_type":"code","source":["## Read the .docs file\n","CorpusDocs = {}\n","\n","with open('/content/nfcorpus/train.docs', 'r', encoding='utf-8') as file:\n","    lines = file.readlines()\n","\n","    for line in lines:\n","        line = line.strip()\n","        doc_id, doc_text = line.split('\\t')\n","        CorpusDocs[doc_id] = doc_text\n","\n","# Now you have the documents stored in the 'documents' dictionary"],"metadata":{"id":"iPwy547eLF0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read any queries\n","CorpusQueries = {}\n","\n","with open('/content/nfcorpus/train.all.queries', 'r', encoding='utf-8') as file:\n","    lines = file.readlines()\n","\n","    for line in lines:\n","        line = line.strip()\n","        query_id, query_text = line.split('\\t')\n","        CorpusQueries[query_id] = query_text\n","\n","# Now you have the queries stored in the 'queries' dictionary\n","CorpusQueries = dict(sorted(CorpusQueries.items()))\n"],"metadata":{"id":"jNVuEThNLHF9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read relevance judgements\n","\n","CorpusRelJudgements = {}\n","\n","with open('/content/nfcorpus/train.3-2-1.qrel', 'r', encoding='utf-8') as file:\n","    lines = file.readlines()\n","\n","    for line in lines:\n","        line = line.strip()\n","        query_id, _, doc_id, relevance = line.split('\\t')\n","\n","        if query_id not in CorpusRelJudgements:\n","            CorpusRelJudgements[query_id] = {}\n","\n","        CorpusRelJudgements[query_id][doc_id] = int(relevance)\n","\n","# Now you have the relevance judgments stored in the 'relevance_judgments' dictionary\n","corpus_result_que = {}\n","for i in CorpusRelJudgements:\n","  res = []\n","  for j in CorpusRelJudgements[i]:\n","    res.append(j)\n","  corpus_result_que[i] = res\n","\n","corpus_result_que = dict(sorted(corpus_result_que.items()))"],"metadata":{"id":"dF0J6FaZLIcE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"wjQiUPIiLCHR"}},{"cell_type":"markdown","source":["## Porter Stemmer Preprocess"],"metadata":{"id":"8q0jbJxd6vBE"}},{"cell_type":"code","source":["def preprocessPorterStemmerOnly(docs):\n","    # docs is a dict {id: doc}\n","    porter_stemmer = PorterStemmer()\n","    terms = set()\n","    modified_docs = {}\n","\n","    for i, doc in enumerate(docs):\n","        modified_doc = docs[doc].lower()\n","        modified_doc = re.sub(r'[^a-z0-9\\s]', '', modified_doc)\n","        # use nltk to tokenize and filter the punctuation\n","        words = word_tokenize(modified_doc)\n","        sentence = \"\"\n","        for word in words:\n","            if word.isdigit() or word.isnumeric():\n","                continue\n","            if not word.isalnum():\n","                continue\n","            word = porter_stemmer.stem(word)\n","            terms.add(word)\n","            sentence += \" \" + word\n","        modified_docs[doc] = sentence\n","\n","    return modified_docs, terms"],"metadata":{"id":"GmvsA09RLDDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Porter Stemmer version of the Preprocess\n","def preprocessPorterStemmer(docs):\n","    # docs is a dict {id: doc}\n","    porter_stemmer = PorterStemmer()\n","    stop_words = set(stopwords.words('english'))\n","    terms = set()\n","    modified_docs = {}\n","\n","    for i, doc in enumerate(docs):\n","        modified_doc = docs[doc].lower()\n","        modified_doc = re.sub(r'[^a-z0-9\\s]', '', modified_doc)\n","        # use nltk to tokenize and filter the punctuation\n","        words = word_tokenize(modified_doc)\n","        sentence = \"\"\n","        for word in words:\n","            if word not in string.punctuation and word not in stop_words:\n","                if word.isdigit() or word.isnumeric():\n","                    continue\n","                if not word.isalnum():\n","                    continue\n","                word = porter_stemmer.stem(word)\n","                terms.add(word)\n","                sentence += \" \" + word\n","        modified_docs[doc] = sentence\n","\n","    return modified_docs, terms\n","\n","# docs, terms = preprocessPorterStemmer(docs)"],"metadata":{"id":"w6T11yEh0SGo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lemmatizer Preprocess"],"metadata":{"id":"tjk1V09v6zyJ"}},{"cell_type":"code","source":["def preprocessLemmatizerOnly(docs):\n","    # docs is a dict {id: doc}\n","    lemmatizer = WordNetLemmatizer()\n","    terms = set()\n","    modified_docs = {}\n","\n","    for i, doc in enumerate(docs):\n","        modified_doc = docs[doc].lower()\n","        modified_doc = re.sub(r'[^a-z0-9\\s]', '', modified_doc)\n","        # use nltk to tokenize and filter the punctuation\n","        words = word_tokenize(modified_doc)\n","        sentence = \"\"\n","        for word in words:\n","            if word.isdigit() or word.isnumeric():\n","                continue\n","            if not word.isalnum():\n","                continue\n","            word = lemmatizer.lemmatize(word)\n","            terms.add(word)\n","            sentence += \" \" + word\n","        modified_docs[doc] = sentence\n","\n","    return modified_docs, terms\n"],"metadata":{"id":"BFsaYjTN0Vtq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lemmatizer Version of Preprocess\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import string\n","\n","def preprocessLematizer(docs):\n","    # docs is a dict {id: doc}\n","    lemmatizer = WordNetLemmatizer()\n","    stop_words = set(stopwords.words('english'))\n","    terms = set()\n","    modified_docs = {}\n","\n","    for i, doc in enumerate(docs):\n","        modified_doc = docs[doc].lower()\n","        modified_doc = re.sub(r'[^a-z0-9\\s]', '', modified_doc)\n","        # use nltk to tokenize and filter the punctuation\n","        words = word_tokenize(modified_doc)\n","        sentence = \"\"\n","        for word in words:\n","            if word not in string.punctuation and word not in stop_words:\n","                if word.isdigit() or word.isnumeric():\n","                    continue\n","                if not word.isalnum():\n","                    continue\n","                word = lemmatizer.lemmatize(word)\n","                terms.add(word)\n","                sentence += \" \" + word\n","        modified_docs[doc] = sentence\n","\n","    return modified_docs, terms\n","\n","# docs, terms = preprocessLematizer(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2oFKPnmE0YSg","outputId":"ede20cdb-bf92-4ecc-cf7f-b43acebe8b00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## Indexing"],"metadata":{"id":"lluYmiZF63EM"}},{"cell_type":"code","source":["# Create the indexing, order term and inverse order term\n","\n","def indexing(docs, terms):\n","    # docs is a dict which {id: doc}\n","    # terms is the set word not include punctuation\n","    # indexs is a list {'word': doc_id} word can duplicate\n","    # order_term is a dict {'word': order in term}\n","    # inverse_order_term {order in term: 'word'}\n","\n","    index = []\n","    order_term = {term: index for index, term in enumerate(terms)}\n","    inverse_order_term = {index: term for term, index in order_term.items()}\n","    for doc_id, doc in docs.items():\n","        words = re.findall(r'\\b\\w+\\b', doc)\n","        words = [word for word in words if word in terms]\n","        for word in words:\n","            index.append({'word': word, 'doc_id':doc_id})\n","\n","    return index, order_term, inverse_order_term\n","\n","# indexs, order_term, inverse_order_term = indexing(docs, terms)"],"metadata":{"id":"GHxn1u010aAL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Find Max ID"],"metadata":{"id":"zuH7m4nS65BF"}},{"cell_type":"code","source":["def find_max_id(docs):\n","    max_id = 0\n","    for doc in docs:\n","        max_id = max(max_id, doc)\n","    return max_id\n","\n","# max_id = find_max_id(docs)"],"metadata":{"id":"615PlnmG0bcK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Adding Frequencies"],"metadata":{"id":"hG5OD-_C6_MJ"}},{"cell_type":"code","source":["# Adding Frequency to the index table\n","\n","def adding_frequency(docs, terms, sorted_indexs):\n","    # new_sorted_indexs is a dict {'word', doc_id, frequency in doc_id}\n","    new_sorted_indexs = []\n","    for entry in sorted_indexs:\n","        word = entry['word']\n","        doc_id = entry['doc_id']\n","        frequency = docs[doc_id].count(word)\n","        entry['frequency'] = frequency\n","        new_sorted_indexs.append(entry)\n","    return new_sorted_indexs\n","\n","# new_sorted_indexs = adding_frequency(docs, terms, sorted_indexs)"],"metadata":{"id":"19Rh7m0J0clY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build Vocabs"],"metadata":{"id":"6mBlcWRr7EAz"}},{"cell_type":"code","source":["# Build the vocabs\n","\n","def build_vocab(docs, terms, new_sorted_indexs):\n","    # vocab {'word': word, 'number of doc include': number of document, 'frequency': frequency, 'position' : a set position in new_sorted_indexs}\n","\n","\n","    vocab = {}\n","    for i, entry in enumerate(new_sorted_indexs):\n","        word = entry['word']\n","        doc_id = entry['doc_id']\n","        if word not in vocab:\n","            vocab[word] = {'number of doc include': 0, 'frequency': 0, 'position': set()}\n","        if doc_id not in docs:\n","            continue\n","        vocab[word]['number of doc include'] += 1\n","        vocab[word]['frequency'] += entry['frequency']\n","        vocab[word]['position'].add(i)\n","\n","    return vocab\n","\n","# vocab = build_vocab(docs, terms, sorted_indexs)"],"metadata":{"id":"Il9GdPy40d3i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## IDF"],"metadata":{"id":"W48C3smB7GvY"}},{"cell_type":"code","source":["# Calculate the IDF of a term with multiple options\n","\n","def calculate_idf(docs, terms, vocab, new_sorted_indexs, option):\n","    if option == 'inverse':\n","        calculate_inverse_idf(vocab, len(docs))\n","    elif option == 'count':\n","        calculate_count_idf(vocab)\n","    elif option == 'square':\n","        calculate_inverse_idf(vocab, len(docs))\n","        calculate_square_idf(vocab)\n","    elif option == 'proba':\n","        calculate_proba_idf(vocab, len(docs))\n","    elif option == 'frequency':\n","        calculate_frequency_idf(vocab, new_sorted_indexs)\n","    return vocab\n","\n","def calculate_inverse_idf(vocab, total_docs):\n","    for word in vocab:\n","        num_docs = vocab[word]['number of doc include']\n","        if num_docs > 0:\n","            idf = math.log(total_docs / num_docs)\n","            vocab[word]['idf'] = idf\n","\n","def calculate_count_idf(vocab):\n","    for word in vocab:\n","        num_docs = vocab[word]['number of doc include']\n","        if num_docs > 0:\n","            vocab[word]['idf'] = 1 / num_docs\n","\n","def calculate_square_idf(vocab):\n","    for word in vocab:\n","        if 'idf' in vocab[word]:\n","            idf = vocab[word]['idf']\n","            square_idf = idf * idf\n","            vocab[word]['idf'] = square_idf\n","\n","def calculate_proba_idf(vocab, total_docs):\n","    for word in vocab:\n","        num_docs = vocab[word]['number of doc include']\n","        if num_docs > 0:\n","            idf = math.log((total_docs - num_docs) / num_docs)\n","            vocab[word]['idf'] = idf\n","\n","def calculate_frequency_idf(vocab, new_sorted_indexs):\n","    for word in vocab:\n","        num_docs = vocab[word]['number of doc include']\n","        if num_docs > 0:\n","            idf = 1 / num_docs\n","            vocab[word]['idf'] = idf"],"metadata":{"id":"1rIseUpi0fNk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TF"],"metadata":{"id":"bG6EFFah7JVs"}},{"cell_type":"code","source":["# Calculate the TF of a term with multiple options\n","\n","def calculate_tf(docs, terms, vocab, new_sorted_indexs, option):\n","    if option == 'binary':\n","        calculate_binary_tf(new_sorted_indexs)\n","    elif option == 'count':\n","        calculate_count_tf(new_sorted_indexs)\n","    elif option == 'max':\n","        calculate_max_tf(new_sorted_indexs)\n","    elif option == 'augmented':\n","        calculate_augmented_tf(new_sorted_indexs)\n","    elif option == 'logarithmic':\n","        calculate_logarithmic_tf(new_sorted_indexs)\n","    return new_sorted_indexs\n","\n","def calculate_binary_tf(new_sorted_indexs):\n","    for entry in new_sorted_indexs:\n","        entry['tf'] = 1\n","\n","def calculate_count_tf(new_sorted_indexs):\n","    for entry in new_sorted_indexs:\n","        entry['tf'] = entry['frequency']\n","\n","def calculate_max_tf(new_sorted_indexs):\n","    max_tf = max(entry['frequency'] for entry in new_sorted_indexs)\n","    for entry in new_sorted_indexs:\n","        entry['tf'] = entry['frequency'] / max_tf\n","\n","def calculate_augmented_tf(new_sorted_indexs):\n","    max_tf = max(entry['frequency'] for entry in new_sorted_indexs)\n","    for entry in new_sorted_indexs:\n","        entry['tf'] = 0.5 + 0.5 * entry['frequency'] / max_tf\n","\n","def calculate_logarithmic_tf(new_sorted_indexs):\n","    for entry in new_sorted_indexs:\n","        entry['tf'] = 1 + math.log(entry['frequency'])"],"metadata":{"id":"s2shmkax0g26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculate the weight"],"metadata":{"id":"uZ72T5gv7LZd"}},{"cell_type":"code","source":["def calculate_w(docs, terms, new_idf_vocab, new_tf_sorted_indexs, option):\n","    # w = idf * tf\n","    # tf in new_sorted_indexs\n","    # idf in vocab\n","\n","    for entry in new_tf_sorted_indexs:\n","        word = entry['word']\n","        doc_id = entry['doc_id']\n","        if word in new_idf_vocab:\n","            tf = entry['tf']\n","            idf = new_idf_vocab[word]['idf']\n","        entry['w'] = idf * tf\n","    if option == 'sum':\n","        calculate_sum_w(new_tf_sorted_indexs)\n","    elif option == 'cosine':\n","        calculate_cosine_w(new_tf_sorted_indexs)\n","    elif option == '**4':\n","        calculate_power_4_w(new_tf_sorted_indexs)\n","    elif option == 'max':\n","        calculate_max_w(new_tf_sorted_indexs)\n","\n","    return new_tf_sorted_indexs\n","\n","def calculate_sum_w(new_tf_sorted_indexs):\n","    doc_weights = {}\n","    for entry in new_tf_sorted_indexs:\n","        doc_id = entry['doc_id']\n","        w = entry['w']\n","        if doc_id in doc_weights:\n","            doc_weights[doc_id] += w\n","        else:\n","            doc_weights[doc_id] = w\n","    for entry in new_tf_sorted_indexs:\n","        doc_id = entry['doc_id']\n","        entry['w'] /= doc_weights[doc_id]\n","\n","def calculate_cosine_w(new_tf_sorted_indexs):\n","    doc_lengths = {}\n","    for entry in new_tf_sorted_indexs:\n","        doc_id = entry['doc_id']\n","        w = entry['w']\n","        if doc_id in doc_lengths:\n","            doc_lengths[doc_id] += w ** 2\n","        else:\n","            doc_lengths[doc_id] = w ** 2\n","    for entry in new_tf_sorted_indexs:\n","        doc_id = entry['doc_id']\n","        w = entry['w']\n","        length = math.sqrt(doc_lengths[doc_id])\n","        entry['w'] = w / length if length != 0 else 0.0\n","\n","def calculate_power_4_w(new_tf_sorted_indexs):\n","    doc_lengths = {}\n","    for entry in new_tf_sorted_indexs:\n","        doc_id = entry['doc_id']\n","        w = entry['w']\n","        if doc_id in doc_lengths:\n","            doc_lengths[doc_id] += w ** 4\n","        else:\n","            doc_lengths[doc_id] = w ** 4\n","    for entry in new_tf_sorted_indexs:\n","        doc_id = entry['doc_id']\n","        w = entry['w']\n","        length = doc_lengths[doc_id]\n","        entry['w'] = w / length if length != 0 else 0.0\n","\n","def calculate_max_w(new_tf_sorted_indexs):\n","    doc_max_w = {}\n","    for entry in new_tf_sorted_indexs:\n","        doc_id = entry['doc_id']\n","        w = entry['w']\n","\n","        if doc_id in doc_max_w:\n","            doc_max_w[doc_id] = max(doc_max_w[doc_id], w)\n","        else:\n","            doc_max_w[doc_id] = w\n","    for entry in new_tf_sorted_indexs:\n","        doc_id = entry['doc_id']\n","        w = entry['w']\n","        max_w = doc_max_w[doc_id]\n","        entry['w'] = w / max_w if max_w != 0 else 0.0"],"metadata":{"id":"QLyGFCGh0icn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculate the TF-IDF Score of the query"],"metadata":{"id":"jk0R3OtO7OGv"}},{"cell_type":"code","source":["def calculate_tf_idf_query(query, new_tf_sorted_indexs, new_idf_vocab, order_term, option_norm):\n","    tf_idf_query = {}\n","    query = query.lower()\n","    query = re.findall(r'\\b\\w+\\b', query)\n","    query = [word for word in query if word in terms]\n","    # tf number appearance\n","    # idf get in new_idf_vocab\n","    # option norm: 'sum', 'cosine', '**4', 'max'\n","\n","    term_frequencies = {}\n","    for term in query:\n","        term_frequencies[term] = term_frequencies.get(term, 0) + 1\n","\n","    for term, frequency in term_frequencies.items():\n","        if term in order_term:\n","            tf = frequency\n","            idf = new_idf_vocab[term]['idf']\n","            tf_idf_query[term] = tf * idf\n","\n","    if option_norm == 'sum':\n","        tf_idf_sum = sum(tf_idf_query.values())\n","        if tf_idf_sum != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_sum for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'cosine':\n","        tf_idf_norm = math.sqrt(sum([tf_idf ** 2 for tf_idf in tf_idf_query.values()]))\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == '**4':\n","        tf_idf_norm = sum([tf_idf ** 4 for tf_idf in tf_idf_query.values()])\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'max':\n","        tf_idf_max = max(tf_idf_query.values())\n","        if tf_idf_max != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_max for term, tf_idf in tf_idf_query.items()}\n","    return tf_idf_query"],"metadata":{"id":"3TJ6VLj80j5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_tf_idf_queryPorterStemmerOnly(query, new_tf_sorted_indexs, new_idf_vocab, order_term, option_norm):\n","    tf_idf_query = {}\n","    query = query.lower()\n","    query = re.findall(r'\\b\\w+\\b', query)\n","\n","    porter_stemmer = PorterStemmer()\n","    query = [porter_stemmer.stem(word) for word in query]\n","\n","    # tf number appearance\n","    # idf get in new_idf_vocab\n","    # option norm: 'sum', 'cosine', '**4', 'max'\n","\n","    term_frequencies = {}\n","    for term in query:\n","        term_frequencies[term] = term_frequencies.get(term, 0) + 1\n","\n","    for term, frequency in term_frequencies.items():\n","        if term in order_term:\n","            tf = frequency\n","            idf = new_idf_vocab[term]['idf']\n","            tf_idf_query[term] = tf * idf\n","\n","    if option_norm == 'sum':\n","        tf_idf_sum = sum(tf_idf_query.values())\n","        if tf_idf_sum != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_sum for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'cosine':\n","        tf_idf_norm = math.sqrt(sum([tf_idf ** 2 for tf_idf in tf_idf_query.values()]))\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == '**4':\n","        tf_idf_norm = sum([tf_idf ** 4 for tf_idf in tf_idf_query.values()])\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'max':\n","        tf_idf_max = max(tf_idf_query.values())\n","        if tf_idf_max != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_max for term, tf_idf in tf_idf_query.items()}\n","    return tf_idf_query"],"metadata":{"id":"MFObk05q0lwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_tf_idf_queryPorterStemmer(query, new_tf_sorted_indexs, new_idf_vocab, order_term, option_norm):\n","    tf_idf_query = {}\n","    query = query.lower()\n","    query = re.findall(r'\\b\\w+\\b', query)\n","\n","    porter_stemmer = PorterStemmer()\n","    stop_words = set(stopwords.words('english'))\n","    query = [porter_stemmer.stem(word) for word in query if word not in stop_words]\n","\n","    # tf number appearance\n","    # idf get in new_idf_vocab\n","    # option norm: 'sum', 'cosine', '**4', 'max'\n","\n","    term_frequencies = {}\n","    for term in query:\n","        term_frequencies[term] = term_frequencies.get(term, 0) + 1\n","\n","    for term, frequency in term_frequencies.items():\n","        if term in order_term:\n","            tf = frequency\n","            idf = new_idf_vocab[term]['idf']\n","            tf_idf_query[term] = tf * idf\n","\n","    if option_norm == 'sum':\n","        tf_idf_sum = sum(tf_idf_query.values())\n","        if tf_idf_sum != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_sum for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'cosine':\n","        tf_idf_norm = math.sqrt(sum([tf_idf ** 2 for tf_idf in tf_idf_query.values()]))\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == '**4':\n","        tf_idf_norm = sum([tf_idf ** 4 for tf_idf in tf_idf_query.values()])\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'max':\n","        tf_idf_max = max(tf_idf_query.values())\n","        if tf_idf_max != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_max for term, tf_idf in tf_idf_query.items()}\n","    return tf_idf_query\n","\n","# query = \"what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft .\"\n","# tf_idf_query = calculate_tf_idf_query(query, new_tf_sorted_indexs, new_idf_vocab, order_term, 'sum')"],"metadata":{"id":"wN4M5MXd03xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_tf_idf_queryLemmatizerOnly(query, new_tf_sorted_indexs, new_idf_vocab, order_term, option_norm):\n","    tf_idf_query = {}\n","    query = query.lower()\n","    query = re.findall(r'\\b\\w+\\b', query)\n","\n","    lemmatizer = WordNetLemmatizer()\n","    query = [lemmatizer.lemmatize(word) for word in query]\n","\n","    # tf number appearance\n","    # idf get in new_idf_vocab\n","    # option norm: 'sum', 'cosine', '**4', 'max'\n","\n","    term_frequencies = {}\n","    for term in query:\n","        term_frequencies[term] = term_frequencies.get(term, 0) + 1\n","\n","    for term, frequency in term_frequencies.items():\n","        if term in order_term:\n","            tf = frequency\n","            idf = new_idf_vocab[term]['idf']\n","            tf_idf_query[term] = tf * idf\n","\n","    if option_norm == 'sum':\n","        tf_idf_sum = sum(tf_idf_query.values())\n","        if tf_idf_sum != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_sum for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'cosine':\n","        tf_idf_norm = math.sqrt(sum([tf_idf ** 2 for tf_idf in tf_idf_query.values()]))\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == '**4':\n","        tf_idf_norm = sum([tf_idf ** 4 for tf_idf in tf_idf_query.values()])\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'max':\n","        tf_idf_max = max(tf_idf_query.values())\n","        if tf_idf_max != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_max for term, tf_idf in tf_idf_query.items()}\n","    return tf_idf_query\n"],"metadata":{"id":"7HnGGgm70-li"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_tf_idf_queryLemmatizer(query, new_tf_sorted_indexs, new_idf_vocab, order_term, option_norm):\n","    tf_idf_query = {}\n","    query = query.lower()\n","    query = re.findall(r'\\b\\w+\\b', query)\n","\n","    lemmatizer = WordNetLemmatizer()\n","    stop_words = set(stopwords.words('english'))\n","    query = [lemmatizer.lemmatize(word) for word in query if word not in stop_words]\n","\n","    # tf number appearance\n","    # idf get in new_idf_vocab\n","    # option norm: 'sum', 'cosine', '**4', 'max'\n","\n","    term_frequencies = {}\n","    for term in query:\n","        term_frequencies[term] = term_frequencies.get(term, 0) + 1\n","\n","    for term, frequency in term_frequencies.items():\n","        if term in order_term:\n","            tf = frequency\n","            idf = new_idf_vocab[term]['idf']\n","            tf_idf_query[term] = tf * idf\n","\n","    if option_norm == 'sum':\n","        tf_idf_sum = sum(tf_idf_query.values())\n","        if tf_idf_sum != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_sum for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'cosine':\n","        tf_idf_norm = math.sqrt(sum([tf_idf ** 2 for tf_idf in tf_idf_query.values()]))\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == '**4':\n","        tf_idf_norm = sum([tf_idf ** 4 for tf_idf in tf_idf_query.values()])\n","        if tf_idf_norm != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_norm for term, tf_idf in tf_idf_query.items()}\n","    elif option_norm == 'max':\n","        tf_idf_max = max(tf_idf_query.values())\n","        if tf_idf_max != 0:\n","            tf_idf_query = {term: tf_idf / tf_idf_max for term, tf_idf in tf_idf_query.items()}\n","    return tf_idf_query\n","\n","# query = \"what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft .\"\n","# tf_idf_query = calculate_tf_idf_queryLemmatizer(query, new_tf_sorted_indexs, new_idf_vocab, order_term, 'sum')"],"metadata":{"id":"OeuDFHfU0_9K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Information Retrieval Function (Finding Relevant Documents)"],"metadata":{"id":"GYL10ic-7VAw"}},{"cell_type":"code","source":["def information_retrieval(tf_idf_query, new_idf_vocab, new_w_sorted_indexs):\n","    result = {}\n","    for term, query_weight in tf_idf_query.items():\n","        if term not in new_idf_vocab:\n","            continue\n","        # get doc_id by new_idf_vocab['position']\n","        # get w by new_w_sortd_indexs[doc_id]\n","        positions = list(new_idf_vocab[term]['position'])\n","        for position in positions:\n","            entry = new_w_sorted_indexs[position]\n","            doc_id = entry['doc_id']\n","            doc_weight = entry['w']\n","            retrieval_score = query_weight * doc_weight\n","            if doc_id in result:\n","                result[doc_id] += retrieval_score\n","            else:\n","                result[doc_id] = retrieval_score\n","    return result\n","\n","# retrieval_scores = information_retrieval(tf_idf_query,new_idf_vocab, new_tf_sorted_indexs)\n","# sorted_results = sorted(retrieval_scores.items(), key=lambda x: x[1], reverse=True)\n","# sorted_results[:29]"],"metadata":{"id":"tk4kFKBz1JOl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculate the AP/mAP of all the queries"],"metadata":{"id":"tgcaZg1R7YMT"}},{"cell_type":"code","source":["def precision(y_true, y_pred):\n","  prec_list = []\n","  rec_list = []\n","  count = 0\n","  for i in range(len(y_pred)):\n","    if y_pred[i] in y_true:\n","      count += 1\n","      prec_list.append(count/(i+1))\n","      rec_list.append(count/len(y_true))\n","  return prec_list, rec_list\n","# list prec nội suy nè\n","def prec_in(prec_list, rec_list):\n","  prec_list_in = []\n","  for i in range(11):\n","    rec = i/10\n","    # prec_rec = max(rec->all)\n","    # lấy max từ prec_rec_in\n","    prec_rec_in = next((x[0] for x in enumerate(rec_list) if x[1] >= rec),-1)\n","    # print('rec_in:', prec_rec_in)\n","    prec_rec = 0\n","    if prec_rec_in != -1:\n","      prec_rec = max(prec_list[prec_rec_in:])\n","    prec_list_in.append(prec_rec)\n","  return prec_list_in"],"metadata":{"id":"nIn3V_Hf1KvI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def MAP(list_y_true, list_y_pred, noisuy=True):\n","    common_keys = set(list_y_true.keys()).intersection(set(list_y_pred.keys()))\n","    result_dict = {}\n","\n","    if noisuy:\n","        prec_in_data = {}\n","        for key in common_keys:\n","            prec_list, rec_list = precision(list_y_true[key], list_y_pred[key])\n","            prec_in_list = prec_in(prec_list, rec_list)\n","            # print('prec_list', prec_list)\n","            # print('rec_list', rec_list)\n","            # print('prec_list_in', prec_in_list)\n","            result_dict[key] = sum(prec_in_list) / len(prec_in_list)\n","            prec_in_data[key] = prec_in_list\n","        return prec_in_data, result_dict, sum(result_dict.values()) / len(result_dict.values())\n","    else:\n","        for key in common_keys:\n","            prec_list, rec_list = precision(list_y_true[key], list_y_pred[key])\n","            if len(prec_list) != 0:\n","                result_dict[key] = sum(prec_list) / len(prec_list)\n","            else:\n","                result_dict[key] = 0\n","\n","    return result_dict, sum(result_dict.values()) / len(result_dict.values())"],"metadata":{"id":"MDudAHYo1N9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## P and R ver chang"],"metadata":{"id":"0mMa3inb7ZO8"}},{"cell_type":"code","source":["def count_P_R(list_y_true, list_y_pred):\n","  common_keys = set(list_y_true.keys()).intersection(set(list_y_pred.keys()))\n","  result_P_dict = {}\n","  result_R_dict = {}\n","  for key in common_keys:\n","      count = 0\n","      for i in range(len(list_y_pred[key])):\n","        if list_y_pred[key][i] in list_y_true[key]:\n","          count += 1\n","      if len(list_y_pred[key])!=0:\n","        result_P_dict[key] = count / len(list_y_pred[key])\n","      else:\n","        result_P_dict[key] = 0\n","      result_R_dict[key] = count / len(list_y_true[key])\n","  return result_P_dict, result_R_dict, sum(result_P_dict.values()) / len(result_P_dict.values()), sum(result_R_dict.values()) / len(result_R_dict.values())"],"metadata":{"id":"CgbOXJAe1PiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trec_11(prec_in_data):\n","  trec_11 = {}\n","  for i in range(11):\n","    trec_11[i/10] = 0\n","    for j in prec_in_data:\n","      trec_11[i/10]+=prec_in_data[j][i]/len(prec_in_data)\n","  return trec_11"],"metadata":{"id":"QHHxvjv61RRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Porter Stemmer"],"metadata":{"id":"em8Sv8XP-6jC"}},{"cell_type":"markdown","source":["## Porter Stem + Remove Stopwords"],"metadata":{"id":"74MrGuz8-8gZ"}},{"cell_type":"code","source":["import time\n","\n","# Measure total execution time\n","start_time = time.time()\n","\n","docs, terms = preprocessPorterStemmer(CorpusDocs)\n","indexs, order_term, inverse_order_term = indexing(docs, terms)\n","\n","print(len(indexs))\n","\n","## Sorting the index list in alphabetical order\n","sorted_indexs = sorted(indexs, key=lambda x: x['word'])\n","## Adding the frequencies to the terms in the index table\n","new_sorted_indexs = adding_frequency(docs, terms, sorted_indexs)\n","# Build the vocab table using the docs, terms and the sorted indexes\n","vocab = build_vocab(docs, terms, sorted_indexs)\n","\n","# Calculate total execution time\n","total_timePSStopIndex = time.time() - start_time\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timePSStopIndex)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5mtLx7i1Slo","outputId":"9a16e6ba-de37-4750-b8fc-cd02ac60f5e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["527828\n","Total execution time: 14.156087398529053\n"]}]},{"cell_type":"code","source":["print(f'number of word in terms: {len(terms)}')\n","print(f'number of word in indexs: {len(indexs)}')\n","print(f'number of document: {len(docs)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJEy6e7_5tB1","outputId":"df5cf4ac-c479-41c5-d889-820c9cb0140f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of word in terms: 20978\n","number of word in indexs: 527828\n","number of document: 3612\n"]}]},{"cell_type":"code","source":["new_tf_sorted_indexs = calculate_tf(docs, terms, vocab, sorted_indexs, 'binary')\n","new_idf_vocab = calculate_idf(docs, terms, vocab, sorted_indexs, 'inverse')\n","new_w_sorted_indexs = calculate_w(docs, terms, new_idf_vocab, new_tf_sorted_indexs, 'cosine')\n","\n","retrievalResult = dict()\n","\n","start_time = time.time()\n","\n","for key, value in CorpusQueries.items():\n","    valueTFIDF = calculate_tf_idf_queryPorterStemmer(value, new_tf_sorted_indexs, new_idf_vocab, order_term, 'cosine')\n","\n","    retrieval_scores = information_retrieval(valueTFIDF,new_idf_vocab, new_tf_sorted_indexs)\n","    sorted_results = sorted(retrieval_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","    retrievalResult[key] = sorted_results\n","\n","# Calculate total execution time\n","total_timePSStopQuery = time.time() - start_time\n","\n","predResult = {}\n","\n","for key, value in retrievalResult.items():\n","    values_first_column = [tup[0] for tup in value]\n","    predResult[key] = values_first_column\n","\n","predResult = dict(sorted(predResult.items()))\n","\n","APTrecList_PorterStem_Stop, listOfAP_PorterStem_Stop_Inter, MAP_PorterStem_Stop_Inter = MAP(corpus_result_que, predResult)\n","listOfAP_PorterStem_Stop_NonInter, MAP_PorterStem_Stop_NonInter = MAP(corpus_result_que, predResult, False)\n","l_p1, l_r1, p1, r1 = count_P_R(corpus_result_que, predResult)\n","# Print total execution time\n","print(\"Total execution time:\", total_timePSStopQuery)\n","print(MAP_PorterStem_Stop_NonInter)\n","print(MAP_PorterStem_Stop_Inter)\n","print(p1, r1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcR8wMAd5yZn","outputId":"794f63e9-ca35-482e-9464-9ba82fe034cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total execution time: 125.09028959274292\n","0.18991752235133255\n","0.20488963478590122\n","0.016181867685635024 0.9529044550417354\n"]}]},{"cell_type":"markdown","source":["## Porter Stem Only"],"metadata":{"id":"Df9HLDqZ_BF1"}},{"cell_type":"code","source":["import time\n","\n","start_time = time.time()\n","\n","docs, terms = preprocessPorterStemmerOnly(CorpusDocs)\n","indexs, order_term, inverse_order_term = indexing(docs, terms)\n","\n","print(len(indexs))\n","\n","## Sorting the index list in alphabetical order\n","sorted_indexs = sorted(indexs, key=lambda x: x['word'])\n","## Adding the frequencies to the terms in the index table\n","new_sorted_indexs = adding_frequency(docs, terms, sorted_indexs)\n","# Build the vocab table using the docs, terms and the sorted indexes\n","vocab = build_vocab(docs, terms, sorted_indexs)\n","\n","# Calculate total execution time\n","total_timePSIndex = time.time() - start_time\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timePSIndex)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mcCDcowX-vvv","outputId":"3ff7f976-90e8-4c4d-89c1-ae4dbc8760b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["528389\n","Total execution time: 11.539645671844482\n"]}]},{"cell_type":"code","source":["print(f'number of word in terms: {len(terms)}')\n","print(f'number of word in indexs: {len(indexs)}')\n","print(f'number of document: {len(docs)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5662AEB_JB7","outputId":"b220178c-6d6c-42ac-bb3b-cfd2bcfbe5a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of word in terms: 21001\n","number of word in indexs: 528389\n","number of document: 3612\n"]}]},{"cell_type":"code","source":["new_tf_sorted_indexs = calculate_tf(docs, terms, vocab, sorted_indexs, 'binary')\n","new_idf_vocab = calculate_idf(docs, terms, vocab, sorted_indexs, 'inverse')\n","new_w_sorted_indexs = calculate_w(docs, terms, new_idf_vocab, new_tf_sorted_indexs, 'cosine')\n","\n","retrievalResult = dict()\n","\n","start_time = time.time()\n","\n","for key, value in CorpusQueries.items():\n","    valueTFIDF = calculate_tf_idf_queryPorterStemmerOnly(value, new_tf_sorted_indexs, new_idf_vocab, order_term, 'cosine')\n","\n","    retrieval_scores = information_retrieval(valueTFIDF,new_idf_vocab, new_tf_sorted_indexs)\n","    sorted_results = sorted(retrieval_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","    retrievalResult[key] = sorted_results\n","\n","total_timePSQuery = time.time() - start_time\n","\n","predResult = {}\n","\n","for key, value in retrievalResult.items():\n","    values_first_column = [tup[0] for tup in value]\n","    predResult[key] = values_first_column\n","\n","predResult = dict(sorted(predResult.items()))\n","\n","APTrecList_PorterStem, listOfAP_PorterStem_Inter, MAP_PorterStem_Inter = MAP(corpus_result_que, predResult)\n","listOfAP_PorterStem_NonInter, MAP_PorterStem_NonInter = MAP(corpus_result_que, predResult, False)\n","l_p2, l_r2, p2, r2 = count_P_R(corpus_result_que, predResult)\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timePSQuery)\n","print(MAP_PorterStem_NonInter)\n","print(MAP_PorterStem_Inter)\n","print(p2, r2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbTeeGXF_NHl","outputId":"f10a7666-9b50-4112-97e2-4f2eb9aad3bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total execution time: 138.7916443347931\n","0.15510611102460825\n","0.1661026659587462\n","0.016170790810072097 0.9534496018264813\n"]}]},{"cell_type":"markdown","source":["# Lemmatizer"],"metadata":{"id":"PyRS6MAnAMyy"}},{"cell_type":"markdown","source":["## Lemmatizer + Remove Stopwords"],"metadata":{"id":"uqXXrrQeJEoG"}},{"cell_type":"code","source":["import time\n","\n","start_time = time.time()\n","\n","docs, terms = preprocessLematizer(CorpusDocs)\n","indexs, order_term, inverse_order_term = indexing(docs, terms)\n","\n","print(len(indexs))\n","\n","## Sorting the index list in alphabetical order\n","sorted_indexs = sorted(indexs, key=lambda x: x['word'])\n","## Adding the frequencies to the terms in the index table\n","new_sorted_indexs = adding_frequency(docs, terms, sorted_indexs)\n","# Build the vocab table using the docs, terms and the sorted indexes\n","vocab = build_vocab(docs, terms, sorted_indexs)\n","\n","# Calculate total execution time\n","total_timeLemmaStopIndex = time.time() - start_time\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timeLemmaStopIndex)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_3q8fEhUAND1","outputId":"f1e7fb03-1413-455f-badf-dc2ced0c2136"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["527828\n","Total execution time: 7.775843858718872\n"]}]},{"cell_type":"code","source":["print(f'number of word in terms: {len(terms)}')\n","print(f'number of word in indexs: {len(indexs)}')\n","print(f'number of document: {len(docs)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xPakuj4ATmX","outputId":"47671aec-be3a-41f3-f378-9d92e9ea6b38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of word in terms: 25925\n","number of word in indexs: 527828\n","number of document: 3612\n"]}]},{"cell_type":"code","source":["new_tf_sorted_indexs = calculate_tf(docs, terms, vocab, sorted_indexs, 'binary')\n","new_idf_vocab = calculate_idf(docs, terms, vocab, sorted_indexs, 'inverse')\n","new_w_sorted_indexs = calculate_w(docs, terms, new_idf_vocab, new_tf_sorted_indexs, 'cosine')\n","\n","retrievalResult = dict()\n","\n","start_time = time.time()\n","\n","for key, value in CorpusQueries.items():\n","    valueTFIDF = calculate_tf_idf_queryLemmatizer(value, new_tf_sorted_indexs, new_idf_vocab, order_term, 'cosine')\n","\n","    retrieval_scores = information_retrieval(valueTFIDF,new_idf_vocab, new_tf_sorted_indexs)\n","    sorted_results = sorted(retrieval_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","    retrievalResult[key] = sorted_results\n","\n","# Calculate total execution time\n","total_timeLemmaStopQuery = time.time() - start_time\n","\n","predResult = {}\n","\n","for key, value in retrievalResult.items():\n","    values_first_column = [tup[0] for tup in value]\n","    predResult[key] = values_first_column\n","\n","# predResult = dict(sorted(predResult.items()))\n","# for i in retrievalResult:\n","#   print(retrievalResult[i])\n","\n","APTrecList_Lemma_Stop, listOfAP_Lemma_Stop_Inter, MAP_Lemma_Stop_Inter = MAP(corpus_result_que, predResult)\n","listOfAP_Lemma_Stop_NonInter, MAP_Lemma_Stop_NonInter = MAP(corpus_result_que, predResult, False)\n","l_p3, l_r3, p3, r3 = count_P_R(corpus_result_que, predResult)\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timeLemmaStopQuery)\n","print(MAP_Lemma_Stop_NonInter)\n","print(MAP_Lemma_Stop_Inter)\n","print(p3, r3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vtGP8twrAVY7","outputId":"77f49ce9-78d6-4525-b7d8-784786c2a564"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total execution time: 85.0072271823883\n","0.19084241355532475\n","0.20524251449026903\n","0.016337289895641895 0.9486093506624861\n"]}]},{"cell_type":"markdown","source":["## Lemma Only"],"metadata":{"id":"3TVWO9XNBAgl"}},{"cell_type":"code","source":["import time\n","\n","start_time = time.time()\n","\n","docs, terms = preprocessLemmatizerOnly(CorpusDocs)\n","indexs, order_term, inverse_order_term = indexing(docs, terms)\n","\n","print(len(indexs))\n","\n","## Sorting the index list in alphabetical order\n","sorted_indexs = sorted(indexs, key=lambda x: x['word'])\n","\n","## Adding the frequencies to the terms in the index table\n","new_sorted_indexs = adding_frequency(docs, terms, sorted_indexs)\n","\n","# Build the vocab table using the docs, terms and the sorted indexes\n","vocab = build_vocab(docs, terms, sorted_indexs)\n","\n","# Calculate total execution time\n","total_timeLemmaIndex = time.time() - start_time\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timeLemmaIndex)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKB1IsaKA_nB","outputId":"2fb49d8f-07d7-407c-a76f-d85ca4876c25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["528389\n","Total execution time: 5.844847917556763\n"]}]},{"cell_type":"code","source":["print(f'number of word in terms: {len(terms)}')\n","print(f'number of word in indexs: {len(indexs)}')\n","print(f'number of document: {len(docs)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOvadCdDBHUL","outputId":"71656040-db3a-4ce4-b377-03a377965d8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of word in terms: 25946\n","number of word in indexs: 528389\n","number of document: 3612\n"]}]},{"cell_type":"code","source":["new_tf_sorted_indexs = calculate_tf(docs, terms, vocab, sorted_indexs, 'binary')\n","new_idf_vocab = calculate_idf(docs, terms, vocab, sorted_indexs, 'inverse')\n","new_w_sorted_indexs = calculate_w(docs, terms, new_idf_vocab, new_tf_sorted_indexs, 'cosine')\n","\n","retrievalResult = dict()\n","\n","start_time = time.time()\n","\n","for key, value in CorpusQueries.items():\n","    valueTFIDF = calculate_tf_idf_queryLemmatizerOnly(value, new_tf_sorted_indexs, new_idf_vocab, order_term, 'cosine')\n","\n","    retrieval_scores = information_retrieval(valueTFIDF,new_idf_vocab, new_tf_sorted_indexs)\n","    sorted_results = sorted(retrieval_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","    retrievalResult[key] = sorted_results\n","\n","total_timeLemmaQuery = time.time() - start_time\n","\n","predResult = {}\n","\n","for key, value in retrievalResult.items():\n","    values_first_column = [tup[0] for tup in value]\n","    predResult[key] = values_first_column\n","\n","predResult = dict(sorted(predResult.items()))\n","\n","APTrecList_Lemma, listOfAP_Lemma_Inter, MAP_Lemma_Inter = MAP(corpus_result_que, predResult)\n","l_p4, l_r4, p4, r4 = count_P_R(corpus_result_que, predResult)\n","listOfAP_Lemma_NonInter, MAP_Lemma_NonInter = MAP(corpus_result_que, predResult, False)\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timeLemmaQuery)\n","print(MAP_Lemma_NonInter)\n","print(MAP_Lemma_Inter)\n","print(p4, r4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4m59bSo-BI7U","outputId":"e52e51a0-4216-48ea-b100-b81f2460049e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total execution time: 87.28621697425842\n","0.15695809779980346\n","0.16757651783652658\n","0.01632231156402499 0.9492199469340202\n"]}]},{"cell_type":"markdown","source":["# No Process"],"metadata":{"id":"tYfXlA4QBYHU"}},{"cell_type":"code","source":["def preprocess(docs):\n","    # docs is a dict {id: doc}\n","    terms = set()\n","    modified_docs = {}\n","\n","    for i, doc in enumerate(docs):\n","        modified_doc = docs[doc].lower()\n","        modified_doc = re.sub(r'[^a-z0-9\\s]', '', modified_doc)\n","        # use nltk to tokenize and filter the punctuation\n","        words = word_tokenize(modified_doc)\n","        sentence = \"\"\n","        for word in words:\n","            if word not in string.punctuation:\n","                terms.add(word)\n","                sentence += \" \" + word\n","        modified_docs[doc] = sentence\n","\n","    return modified_docs, terms"],"metadata":{"id":"vcHjdl5pBY06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","\n","docs, terms = preprocess(CorpusDocs)\n","indexs, order_term, inverse_order_term = indexing(docs, terms)\n","\n","print(len(indexs))\n","\n","## Sorting the index list in alphabetical order\n","sorted_indexs = sorted(indexs, key=lambda x: x['word'])\n","\n","## Adding the frequencies to the terms in the index table\n","new_sorted_indexs = adding_frequency(docs, terms, sorted_indexs)\n","\n","# Build the vocab table using the docs, terms and the sorted indexes\n","vocab = build_vocab(docs, terms, sorted_indexs)\n","\n","# Calculate total execution time\n","total_timeBaseIndex = time.time() - start_time\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timeBaseIndex)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iEml_jdEBaVL","outputId":"136d36f4-85e1-4a75-b34c-edbc2aa5eca5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["528389\n","Total execution time: 4.33671498298645\n"]}]},{"cell_type":"code","source":["print(f'number of word in terms: {len(terms)}')\n","print(f'number of word in indexs: {len(indexs)}')\n","print(f'number of document: {len(docs)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzUcf_1VBenf","outputId":"2e6ec555-8612-428e-a34a-553e51912e63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of word in terms: 28286\n","number of word in indexs: 528389\n","number of document: 3612\n"]}]},{"cell_type":"code","source":["new_tf_sorted_indexs = calculate_tf(docs, terms, vocab, sorted_indexs, 'binary')\n","new_idf_vocab = calculate_idf(docs, terms, vocab, sorted_indexs, 'inverse')\n","new_w_sorted_indexs = calculate_w(docs, terms, new_idf_vocab, new_tf_sorted_indexs, 'cosine')\n","\n","retrievalResult = dict()\n","\n","start_time = time.time()\n","\n","for key, value in CorpusQueries.items():\n","    valueTFIDF = calculate_tf_idf_query(value, new_tf_sorted_indexs, new_idf_vocab, order_term, 'cosine')\n","\n","    retrieval_scores = information_retrieval(valueTFIDF,new_idf_vocab, new_tf_sorted_indexs)\n","    sorted_results = sorted(retrieval_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","    retrievalResult[key] = sorted_results\n","\n","# Calculate total execution time\n","total_timeBaseQuery = time.time() - start_time\n","\n","predResult = {}\n","\n","for key, value in retrievalResult.items():\n","    values_first_column = [tup[0] for tup in value]\n","    predResult[key] = values_first_column\n","\n","predResult = dict(sorted(predResult.items()))\n","\n","APTrecList_Stop, listOfAP_Stop_Inter, MAP_Stop_Inter = MAP(corpus_result_que, predResult)\n","l_p5, l_r5, p5, r5 = count_P_R(corpus_result_que, predResult)\n","listOfAP_Stop_NonInter, MAP_Stop_NonInter = MAP(corpus_result_que, predResult, False)\n","\n","# Print total execution time\n","print(\"Total execution time:\", total_timeBaseQuery)\n","print(MAP_Stop_NonInter)\n","print(MAP_Stop_Inter)\n","print(p5, r5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_XHAxZiBgcY","outputId":"d7fd0e11-ea61-4964-baf2-d39530558171"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total execution time: 72.34215545654297\n","0.15482782185089589\n","0.16287137745323652\n","0.016677748943465517 0.9304974169585449\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Thống kê\n","\n","---\n","\n"],"metadata":{"id":"MWyGsmdXBo8b"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Define your data\n","q1 = [\"PorterStem + Stop\", MAP_PorterStem_Stop_Inter, MAP_PorterStem_Stop_NonInter, p1, r1, total_timePSStopIndex, total_timePSStopQuery]\n","q2 = [\"PorterStem\", MAP_PorterStem_Inter, MAP_PorterStem_NonInter, p2, r2, total_timePSIndex, total_timePSQuery]\n","q3 = [\"Lemma + Stop\", MAP_Lemma_Stop_Inter, MAP_Lemma_Stop_NonInter, p3, r3, total_timeLemmaStopIndex, total_timeLemmaStopQuery]\n","q4 = [\"Lemma\", MAP_Lemma_Inter, MAP_Lemma_NonInter, p4, r4, total_timeLemmaIndex, total_timeLemmaQuery]\n","q5 = [\"No Preprocess\", MAP_Stop_Inter, MAP_Stop_NonInter, p5, r5, total_timeBaseIndex, total_timeBaseQuery]\n","\n","col = [\"Type\", \"Inter\", \"NonInter\", \"P\", \"R\", \"Index Time\", \"Query Time\"]\n","\n","# Create the DataFrame\n","df = pd.DataFrame([q1, q2, q3, q4, q5], columns=col)\n","\n","# Round the numeric columns to 4 decimal places\n","numeric_cols = [\"Inter\", \"NonInter\", \"P\", \"R\", \"Index Time\", \"Query Time\"]\n","df[numeric_cols] = df[numeric_cols].round(4)\n","\n","# Print the rounded DataFrame\n","df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"4EhMY9WEBpXu","outputId":"39d56de9-2ce5-4a97-b14b-24ab6825f2a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                Type   Inter  NonInter       P       R  Index Time  Query Time\n","0  PorterStem + Stop  0.2049    0.1899  0.0162  0.9529     14.1561    125.0903\n","1         PorterStem  0.1661    0.1551  0.0162  0.9534     11.5396    138.7916\n","2       Lemma + Stop  0.2052    0.1908  0.0163  0.9486      7.7758     85.0072\n","3              Lemma  0.1676    0.1570  0.0163  0.9492      5.8448     87.2862\n","4      No Preprocess  0.1629    0.1548  0.0167  0.9305      4.3367     72.3422"],"text/html":["\n","  <div id=\"df-97dfc523-a1fb-4634-a83f-d6e4a732e13c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Type</th>\n","      <th>Inter</th>\n","      <th>NonInter</th>\n","      <th>P</th>\n","      <th>R</th>\n","      <th>Index Time</th>\n","      <th>Query Time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>PorterStem + Stop</td>\n","      <td>0.2049</td>\n","      <td>0.1899</td>\n","      <td>0.0162</td>\n","      <td>0.9529</td>\n","      <td>14.1561</td>\n","      <td>125.0903</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>PorterStem</td>\n","      <td>0.1661</td>\n","      <td>0.1551</td>\n","      <td>0.0162</td>\n","      <td>0.9534</td>\n","      <td>11.5396</td>\n","      <td>138.7916</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Lemma + Stop</td>\n","      <td>0.2052</td>\n","      <td>0.1908</td>\n","      <td>0.0163</td>\n","      <td>0.9486</td>\n","      <td>7.7758</td>\n","      <td>85.0072</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lemma</td>\n","      <td>0.1676</td>\n","      <td>0.1570</td>\n","      <td>0.0163</td>\n","      <td>0.9492</td>\n","      <td>5.8448</td>\n","      <td>87.2862</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>No Preprocess</td>\n","      <td>0.1629</td>\n","      <td>0.1548</td>\n","      <td>0.0167</td>\n","      <td>0.9305</td>\n","      <td>4.3367</td>\n","      <td>72.3422</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97dfc523-a1fb-4634-a83f-d6e4a732e13c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-97dfc523-a1fb-4634-a83f-d6e4a732e13c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-97dfc523-a1fb-4634-a83f-d6e4a732e13c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":79}]}]}